---
title: "Homework-Week-6"
output: html_document
---
#1
```{r}
z.prop.test <- function(p1, p2, p0, n1, n2, alternative = "two.tailed", alpha = 0.05, k = 1000){
 p <- rep(NA, k)
 for (i in 1:k){
  p2 <- NULL
  n2 <- NULL
  
  z <- (p2-p1)/sqrt((p0*(1-p0))*((1/n1)+(1/n2)))
  z1p <- (p1-p0)/(sqrt((p0*(1-p0))/n1))
  if (alternative == "less") {
            p[i] <- pnorm(z, lower.tail = TRUE)
  }
   if (alternative == "greater") {
            p[i] <- pnorm(z, lower.tail = FALSE)}
  if (alternative == "two.tailed") {
    if (z > 0) 
                {
                  p[i] <- 2 * pnorm(z, lower.tail = FALSE)
            }
    if (z < 0) 
                {
                  p[i] <- 2 * pnorm(z, lower.tail = TRUE)
    }
  }
 }
if (p2 == NULL){
  z1p
}
 if (alternative == "less") {
            p[i] <- pnorm(z1p, lower.tail = TRUE)
 }
 if (alternative == "greater") {
            p[i] <- pnorm(z1p, lower.tail = FALSE)
 }
 if (alternative == "two.tailed") {
            if (z1p > 0) 
                {
                  p[i] <- 2 * pnorm(z1p, lower.tail = FALSE)
                }  
            if (z1p < 0) 
                {
                  p[i] <- 2 * pnorm(z1p, lower.tail = TRUE)
                } 
        }

if (n2 == NULL){
  z1p
}
 if (alternative == "less") {
            p[i] <- pnorm(z1p, lower.tail = TRUE)
 }
 if (alternative == "greater") {
            p[i] <- pnorm(z1p, lower.tail = FALSE)
 }
 if (alternative == "two.tailed") {
            if (z1p > 0) 
                {
                  p[i] <- 2 * pnorm(z1p, lower.tail = FALSE)
                }  
            if (z1p < 0) 
                {
                  p[i] <- 2 * pnorm(z1p, lower.tail = TRUE)
                } 
        }
if (n1*p1 > 5){"warning: n*p is less than 5 it fails the assumption that it is normal" }
if (n1*(1-p1) >5){"warning: n*(1-p) is less than it fails the assumption that it is normal"}

a <- 0.05 #this is alpha
cva <- qnorm(1-a/2)
cvofp1<-((p1*(1-p1))/n1)
cvofp2<-((p2*(1-p2))/n2)
cvsqrt<-sqrt(cvofp1+cvofp2)
se<-cva*cvsqrt
p1p2 <- p1+p2
cvlower<- p1p2-se
cvupper <- p1p2+se
cv <- c(cvlower, cvupper)
cv


return(z)
return(cv)
return(p[i])#done after 2
#I'm trying to find out how to get the right responces here but something isn't working. I also dont know how to get the appropreate p value for each of the different casses but I'm still working on this. This homework is kind of hard. I'm still working on part two.
}
```

#2

All of this is just the regular thing not the log part
```{r}
library(curl)
library(ggplot2)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

This is as far as I got as of 2, any changes after this were done after , and anything above this that is changed will have a note saying it was changed after 

```{r}
g <- ggplot(data = d, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g
```
I an a little confused about the geom_text() funtion, I can't find it 
```{r}
d2 <- na.omit(d)
head(d2)
```
```{r}
g <- ggplot(data = d2, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g
```




```{r}
l <- d2$MaxLongevity_m
b <- d2$Brain_Size_Species_Mean
beta1 <- cor(b, l) * (sd(l)/sd(b))
beta1
```
```{r}
fit<-lm(l~b)
summary(fit)
```
I think using this function thing summary you can test the Ho and the Ha, we know that our alpha is 0.1 and our p value for slope is  2.176e-05 which is less than alpha we we reject ho. I think this is how you want us to test this. But I dont know how to set the alpha level.
```{r}
confint(fit, 'b', level=0.90)
```
I think this is the 90% ci for the slope
```{r}
confint(fit, level=0.90)
```
```{r}
ci <- predict(fit, newdata = data.frame(Brain_Size_Species_Mean = d2$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  # for a vector of values
head(ci)
```
```{r}
fit<-lm(l~b)
h_hat <- predict(fit, newdata = data.frame(Brain_Size_Species_Mean = d2$Brain_Size_Species_Mean))
df <- data.frame(cbind(d2$Brain_Size_Species_Mean, d2$MaxLongevity_m, h_hat))
names(df) <- c("x", "y", "yhat")
head(df)



df <- cbind(df, ci)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr")
head(df)
```
```{r}
g <- ggplot(data = df, aes(x = x, y = y))
g <- g + geom_point(alpha = 1/2)
g <- g + geom_line(aes(x = x, y = CIfit), colour = "black")
g <- g + geom_line(aes(x = x, y = CIlwr), colour = "blue")
g <- g + geom_line(aes(x = x, y = CIupr), colour = "blue")
g
```
```{r}
pi <- predict(fit, newdata = data.frame(Brain_Size_Species_Mean = d2$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  # for a vector of values
head(pi)
```
```{r}
df <- cbind(df, pi)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", 
    "PIupr")
head(df)
```
```{r}
g <- g + geom_line(data = df, aes(x = x, y = PIlwr), colour = "red")
g <- g + geom_line(data = df, aes(x = x, y = PIupr), colour = "red")
g
```

```{r}
pi <- predict(fit, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction", 
    level = 0.95)  # for a single value
pi
```
I should only have one here but I have a lot so I dont knowhow to continue but ill try
```{r, eval=FALSE}
df <- data.frame(cbind(d2$Brain_Size_Species_Mean, d2$MaxLongevity_m, h_hat))

df <- cbind(df, pi)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", 
    "PIupr")
head(df)
```
Im not sure how to do this part as of now, so I can't graph the pi for the slope at evaxtly x=800

this part all contains the log part
```{r}
library(curl)
library(ggplot2)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```
```{r}
d2 <- na.omit(d)
head(d2)
```
```{r}
logb <- log(d2$Brain_Size_Species_Mean)
logl <- log(d2$MaxLongevity_m)
```

```{r}
g <- ggplot(data = d2, aes(x = logb, y = logl))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g
```
```{r}
beta1 <- cor(logb, logl) * (sd(logl)/sd(logb))
beta1
```
```{r}
logfit<-lm(logl~logb)
summary(logfit)
```
```{r}
confint(logfit, 'logb', level=0.90)
```
I think this is the 90% ci for the slope
```{r}
confint(logfit, level=0.90)
```
```{r}
ci <- predict(logfit, newdata = data.frame(Brain_Size_Species_Mean = log(d2$Brain_Size_Species_Mean)), interval = "confidence", 
    level = 0.90)  # for a vector of values
head(ci)
```
```{r}
logfit<-lm(logl~logb)
h_hat <- predict(logfit, newdata = data.frame(Brain_Size_Species_Mean = log(d2$Brain_Size_Species_Mean)))
logdf <- data.frame(cbind(log(d2$Brain_Size_Species_Mean), log(d2$MaxLongevity_m), h_hat))
names(logdf) <- c("x", "y", "yhat")
head(logdf)



logdf <- cbind(logdf, ci)
names(logdf) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr")
head(df)
```
```{r}
g <- ggplot(data = logdf, aes(x = x, y = y))
g <- g + geom_point(alpha = 1/2)
g <- g + geom_line(aes(x = x, y = CIfit), colour = "black")
g <- g + geom_line(aes(x = x, y = CIlwr), colour = "blue")
g <- g + geom_line(aes(x = x, y = CIupr), colour = "blue")
g
```
```{r}
pi <- predict(logfit, newdata = data.frame(Brain_Size_Species_Mean = log(d2$Brain_Size_Species_Mean)), interval = "prediction", 
    level = 0.90)  # for a vector of values
head(pi)
```
```{r}
logdf <- cbind(logdf, pi)
names(logdf) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", 
    "PIupr")
head(logdf)
```
```{r}
g <- g + geom_line(data = logdf, aes(x = x, y = PIlwr), colour = "red")
g <- g + geom_line(data = logdf, aes(x = x, y = PIupr), colour = "red")
g
```

```{r, eval=FALSE}
pi <- predict(logfit, newdata = data.frame(log(d2$Brain_Size_Species_Mean) = 800), interval = "prediction", 
    level = 0.90)  # for a single value
pi
```
I dont know what the problem here is
I do not know how to make the legend I cant find the right things to make a color coated with the lines. I'm not sue which one is better, both do not look that normal and both have low p values. I think that the log one might be better because it has a lower standard error with the Residuals




























